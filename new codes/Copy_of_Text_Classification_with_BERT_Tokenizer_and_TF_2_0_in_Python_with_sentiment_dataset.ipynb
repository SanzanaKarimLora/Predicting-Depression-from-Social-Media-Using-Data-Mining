{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Text Classification with BERT Tokenizer and TF 2.0 in Python with sentiment dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3Wdti42XTAT",
        "colab_type": "text"
      },
      "source": [
        " **BERT  (Bidirectional Encoder Representations from Transformers )**\n",
        "\n",
        " 1. BERT provides a pre-trained model for English and Chinese language.\n",
        "\n",
        " 2. BERT is a text representation technique like Word Embeddings. \n",
        "\n",
        " 3. BERT is also a text representation technique which is a fusion of variety of state-of-the-art deep learning algorithms, such as bidirectional encoder LSTM and Transformers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ngmik4cIWltD",
        "colab_type": "text"
      },
      "source": [
        "**Installing and Importing Required Libraries**\n",
        "\n",
        "\n",
        "1.   Before using BERT text representation, need to install BERT for TensorFlow 2.0.\n",
        "\n",
        "2. Executing the following pip commands on terminal to install BERT for \n",
        "TensorFlow 2.0.   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JmfHOhmPNBn",
        "colab_type": "code",
        "outputId": "f42ff38e-5e4e-464d-f42c-d3ebb7f64591",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.13.5)\n",
            "Requirement already satisfied: params-flow>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.7.4)\n",
            "Requirement already satisfied: py-params>=0.7.3 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.9.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (1.18.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (4.38.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRh1qkTlaCOy",
        "colab_type": "text"
      },
      "source": [
        "**TensorFlow 2.0. Google Colab**\n",
        "\n",
        "1. TensorFlow is the dominating Deep Learning framework for Data Scientists and Jupyter Notebook is the go-to tool for Data Scientists.\n",
        "\n",
        "2. I am  running TensorFlow 2.0. Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYD8nFbnPdsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2xXWNLgeMDa",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38sFoF2Cef70",
        "colab_type": "text"
      },
      "source": [
        "**Storing Dataset In my drive **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saAO4eFdPj8a",
        "colab_type": "code",
        "outputId": "3b15a7cd-7513-4cde-c34a-557380503e88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "main_directory = '/content/drive/My Drive/Colab Notebooks/a.csv'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J3WMe36U3qQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaS4Dd37eVUS",
        "colab_type": "text"
      },
      "source": [
        "1. The following script imports the dataset using the read_csv() method of the **Pandas dataframe**.\n",
        "\n",
        "2. The script also prints the shape of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3_yj3t1PutS",
        "colab_type": "code",
        "outputId": "f730cc6d-02f6-4b0f-eea4-018b627498e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "sentiment = pd.read_csv(\"/content/drive/My Drive/soft computing/a.csv\",encoding = 'ISO-8859-1', header=None, names =['label','id','date','query','user','tweet'])\n",
        "\n",
        "sentiment.isnull().values.any()\n",
        "\n",
        "sentiment.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1600000, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYKkRKzQapJv",
        "colab_type": "code",
        "outputId": "d090a555-ad06-47cb-f7a4-14454a8a8a7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "sentiment.head(n=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811372</td>\n",
              "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>joy_wolf</td>\n",
              "      <td>@Kwesidei not the whole crew</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811592</td>\n",
              "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mybirch</td>\n",
              "      <td>Need a hug</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811594</td>\n",
              "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>coZZ</td>\n",
              "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811795</td>\n",
              "      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>2Hood4Hollywood</td>\n",
              "      <td>@Tatiana_K nope they didn't have it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>1467812025</td>\n",
              "      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mimismo</td>\n",
              "      <td>@twittera que me muera ?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  ...                                              tweet\n",
              "0      0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1      0  ...  is upset that he can't update his Facebook by ...\n",
              "2      0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3      0  ...    my whole body feels itchy and like its on fire \n",
              "4      0  ...  @nationwideclass no, it's not behaving at all....\n",
              "5      0  ...                      @Kwesidei not the whole crew \n",
              "6      0  ...                                        Need a hug \n",
              "7      0  ...  @LOLTrish hey  long time no see! Yes.. Rains a...\n",
              "8      0  ...               @Tatiana_K nope they didn't have it \n",
              "9      0  ...                          @twittera que me muera ? \n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6bNfvRtotdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment.drop([\"id\", \"date\", \"query\", \"user\"],\n",
        "          axis=1,\n",
        "          inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFToVPiVexwk",
        "colab_type": "text"
      },
      "source": [
        "Removing those columns which we dont need it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHJWkNIpJi0",
        "colab_type": "code",
        "outputId": "1a674aad-e61b-408f-a5a2-ab2ba202ca32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentiment.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1600000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUCntXh1fCc5",
        "colab_type": "text"
      },
      "source": [
        "The output shows that our dataset has 16,00,000 rows and 2 columns now for next step working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OItl30My7jSm",
        "colab_type": "code",
        "outputId": "6e7ceef6-bd37-409e-964a-44453e1cc796",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "sentiment.head(n=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kwesidei not the whole crew</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>Need a hug</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>@Tatiana_K nope they didn't have it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>@twittera que me muera ?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                              tweet\n",
              "0      0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1      0  is upset that he can't update his Facebook by ...\n",
              "2      0  @Kenichan I dived many times for the ball. Man...\n",
              "3      0    my whole body feels itchy and like its on fire \n",
              "4      0  @nationwideclass no, it's not behaving at all....\n",
              "5      0                      @Kwesidei not the whole crew \n",
              "6      0                                        Need a hug \n",
              "7      0  @LOLTrish hey  long time no see! Yes.. Rains a...\n",
              "8      0               @Tatiana_K nope they didn't have it \n",
              "9      0                          @twittera que me muera ? "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTNfwu76e9Uj",
        "colab_type": "text"
      },
      "source": [
        "**Preprocessing the Dataset**\n",
        "\n",
        "1.  For preprocessing our data to remove any punctuations and special characters, i have to define a function that takes as input a raw tweet from tweet column in dataset and returns the corresponding cleaned tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9WW8M1SS0Yn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(sen):\n",
        "    # Removing html tags\n",
        "    sentence = remove_tags(sen)\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh24SLTES5RO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "    return TAG_RE.sub('', text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwVIer-rS77p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "tweeter = []\n",
        "sentence = list(sentiment['tweet'])\n",
        "for sen in sentence:\n",
        "    tweeter.append(preprocess_text(sen))\n",
        "    #df.append(sentiment)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVYkEv_jS_IP",
        "colab_type": "code",
        "outputId": "04eca104-6cbd-4729-cfaf-ada63e3af9c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(sentiment.columns.values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['label' 'tweet']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_EWGDs1gIY8",
        "colab_type": "text"
      },
      "source": [
        "The tweet column contains text\n",
        " while the label column contains number 0 and 4 for sentiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezs93Md9TG_3",
        "colab_type": "code",
        "outputId": "7d14d80f-3cf9-427f-800a-8c099443d0f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentiment.label.unique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qbYaXcngftn",
        "colab_type": "text"
      },
      "source": [
        "The following script replaces 4 sentiment by 1 and the 0 sentiment by 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV4m8V4bTKAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "y = sentiment['label']\n",
        "\n",
        "y = np.array(list(map(lambda x: 1 if x==4 else 0, y)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wGrjrr78Pfb",
        "colab_type": "code",
        "outputId": "d8c3e561-fd62-418f-925e-19c44e563753",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentiment.label.unique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj6jrxG2gvuR",
        "colab_type": "text"
      },
      "source": [
        "randomly printing a tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IebsFX4DTNAS",
        "colab_type": "code",
        "outputId": "33583156-2131-4198-a0e2-453fccbb26ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tweeter[160000])      #tweeter is the list after preposition happens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " tiffanylue know was listenin to bad habit earlier and started freakin at his part \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXyI4pvChC_P",
        "colab_type": "text"
      },
      "source": [
        "It clearly looks like a negative tweet. Let's just confirm it by printing the corresponding label value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRoti7QLTPq9",
        "colab_type": "code",
        "outputId": "4e2ce8a5-53e5-4a59-9f70-7076c65f8daa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(y[12])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5lvAG5dg1Li",
        "colab_type": "text"
      },
      "source": [
        "Its label is 0 means the tweet is negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbbY65S6hNOb",
        "colab_type": "text"
      },
      "source": [
        "After preprocessing our data and  now ready to create BERT representations from our text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvXiZMnyhdcx",
        "colab_type": "text"
      },
      "source": [
        "**Creating a BERT Tokenizer**\n",
        "\n",
        "In order to use BERT **text embeddings** as input to **train** text classification model, we need to tokenize our text tweet. Tokenization refers to dividing a sentence into individual words. To tokenize our text, we will be using the BERT tokenizer. Look at the following script:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1Xpd0s9TS_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)\n",
        "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7euzLjmiM27",
        "colab_type": "text"
      },
      "source": [
        "In the script above,\n",
        "\n",
        "1. we first create an object of the FullTokenizer class from the bert.bert_tokenization module.\n",
        "\n",
        "2. Next, we create a BERT embedding layer by importing the BERT model from hub.KerasLayer. \n",
        "\n",
        "3. The trainable parameter is set to False, which means that we will not be training the BERT embedding. \n",
        "\n",
        "4. In the next line, we create a BERT vocabulary file in the form a numpy array. \n",
        "\n",
        "5. We then set the text to lowercase \n",
        "\n",
        "6. and finally we pass our vocabulary_file \n",
        "\n",
        "7. and to_lower_case variables to the BertTokenizer object.\n",
        "\n",
        "## we will only be using BERT Tokenizer.\n",
        "\n",
        "we will tokenize a random sentence, as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEeyDPelTWLw",
        "colab_type": "code",
        "outputId": "70fe0cd6-e911-4923-f225-684b14ade5d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.tokenize(\"spring break in plain city it snowing\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['spring', 'break', 'in', 'plain', 'city', 'it', 'snow', '##ing']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faddGp_5jCyU",
        "colab_type": "text"
      },
      "source": [
        "we will get the ids of the tokens using the convert_tokens_to_ids() of the tokenizer object. Look at the following script:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNVVU_N5TZE1",
        "colab_type": "code",
        "outputId": "8b622fef-119b-4cde-c0a5-4ebbe739b40f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"spring break in plain city it snowing\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3500, 3338, 1999, 5810, 2103, 2009, 4586, 2075]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uic9rNy-mFP2",
        "colab_type": "text"
      },
      "source": [
        "As in the tokenizer vocabulary, If thereâ€™s a token that is not present in the vocabulary, the tokenizer will use the special [UNK] token and use its id.\n",
        "Thats why its need to convert it in ids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htMO0IMETb3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_tweet(text_tweeter):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_tweeter))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfqntLDCTeJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_tweet = [tokenize_tweet(tweet) for tweet in tweeter]   #tweeter is list , tweet is column of dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC4W8GWLmgh7",
        "colab_type": "text"
      },
      "source": [
        "**Prerparing Data For Training**\n",
        "\n",
        "1. The tweets in our dataset have varying lengths.\n",
        "\n",
        "2. Some Tweets are very small while others are very long.\n",
        "\n",
        "3. To train the model, the input sentences should be of equal length.\n",
        "\n",
        "4. To create sentences of equal length, one way is to pad the shorter sentences by 0s. \n",
        "\n",
        "5. The other way is to pad sentences within each batch.\n",
        "\n",
        "6. Since we will be training the model in batches, we can pad the sentences within the training batch locally depending upon the length of the longest sentence. To do so, we first need to find the length of each sentence.\n",
        "\n",
        "7. The following script creates a list of lists where each sublist contains tokenized tweet, the label of the tweet and the length of the tweet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4VmZVudTghZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_with_len = [[tweet, y[i], len(tweet)]\n",
        "                 for i, tweet in enumerate(tokenized_tweet)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zydjcFYcnfDk",
        "colab_type": "text"
      },
      "source": [
        "In our dataset, \n",
        "1. the first half of the tweets are negative\n",
        "2. while the last half contains negative reviews. \n",
        "3. Therefore, in order to have both positive and negative tweets in the training batches we need to shuffle the tweets. \n",
        "4. The following script shuffles the data randomly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngyjCZtmTjBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(tweets_with_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LVahQfXn2iu",
        "colab_type": "text"
      },
      "source": [
        "Once the data is shuffled, \n",
        "1. we will sort the data by the length of the reviews.\n",
        "2. To do so, we will use the sort() function of the list and \n",
        "3. will tell it that we want to sort the list with respect to the third item in the sublist i.e. the length of the review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZFQXN1bTnG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_with_len.sort(key=lambda x: x[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCPiQMhsoLR7",
        "colab_type": "text"
      },
      "source": [
        "Once the tweets are sorted by length, we can remove the length attribute from all the tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFxNY9JxTqCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sorted_tweets_labels = [(tweet_lab[0], tweet_lab[1]) for tweet_lab in tweets_with_len]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeiIERCwoXFW",
        "colab_type": "text"
      },
      "source": [
        "Once the tweets are sorted\n",
        "1. we will convert thed dataset so that it can be used to train TensorFlow 2.0 models. \n",
        "2. Run the following code to convert the sorted dataset into a TensorFlow 2.0-compliant input dataset shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmYqFDKeTs0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_tweets_labels, output_types=(tf.int32, tf.int32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHNAqpi8kE8n",
        "colab_type": "text"
      },
      "source": [
        "Finally, \n",
        "1.  we can now pad our dataset for each batch. \n",
        "2. The batch size we are going to use is 32 which means that after processing 32 tweets, the weights of the neural network will be updated. \n",
        "3. To pad the tweetss locally with respect to batches, execute the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra-sVayLTvLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 34\n",
        "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbnWlfIBkFwC",
        "colab_type": "text"
      },
      "source": [
        "Remember, for TPU batch size 128 works like a charm, fast and easily trainable. While for GPU it will throw exhaustion error. So, reducing batch size for GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHEkN1YSo5Dl",
        "colab_type": "text"
      },
      "source": [
        "Let's print the first batch and see how padding has been applied to it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik_ksGVITx0h",
        "colab_type": "code",
        "outputId": "268b9685-0a0e-4678-8b47-05cfa125697a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "next(iter(batched_dataset))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(34, 1), dtype=int32, numpy=\n",
              " array([[ 5983],\n",
              "        [ 6023],\n",
              "        [ 2731],\n",
              "        [ 2074],\n",
              "        [19613],\n",
              "        [ 8404],\n",
              "        [ 2189],\n",
              "        [ 2621],\n",
              "        [ 5030],\n",
              "        [ 8785],\n",
              "        [ 5541],\n",
              "        [ 3892],\n",
              "        [27017],\n",
              "        [ 2498],\n",
              "        [24057],\n",
              "        [ 3985],\n",
              "        [ 2147],\n",
              "        [14978],\n",
              "        [14978],\n",
              "        [ 3649],\n",
              "        [ 3478],\n",
              "        [22708],\n",
              "        [14978],\n",
              "        [19453],\n",
              "        [ 2283],\n",
              "        [ 6928],\n",
              "        [ 4071],\n",
              "        [ 6616],\n",
              "        [ 5457],\n",
              "        [22708],\n",
              "        [14978],\n",
              "        [26316],\n",
              "        [22708],\n",
              "        [22708]], dtype=int32)>, <tf.Tensor: shape=(34,), dtype=int32, numpy=\n",
              " array([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1], dtype=int32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljazpoHYpXLS",
        "colab_type": "text"
      },
      "source": [
        "The above output shows \n",
        "1. the first five and last five padded tweetss. From the last five tweet, we can see that the total number of words in the largest sentence were 21.\n",
        "2. Therefore, in the first five tweets the 0s are added at the end of the sentences so that their total length is also 21.\n",
        " 3.  The padding for the next batch will be different depending upon the size of the largest sentence in the batch.\n",
        "\n",
        "Once we have applied padding to our dataset, the next step is to divide the dataset into test and training sets. \n",
        "\n",
        "We can do that with the help of following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4chYzWOkT1zH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "TOTAL_BATCHES = math.ceil(len(sorted_tweets_labels) / BATCH_SIZE)\n",
        "TEST_BATCHES = TOTAL_BATCHES // 10\n",
        "batched_dataset.shuffle(TOTAL_BATCHES)\n",
        "test_data = batched_dataset.take(TEST_BATCHES)\n",
        "train_data = batched_dataset.skip(TEST_BATCHES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwakKy_mpx22",
        "colab_type": "text"
      },
      "source": [
        "In the code above,\n",
        "1. we first find the total number of batches by dividing the total records by 30.\n",
        "2. Next, 10% of the data is left aside for testing. \n",
        "3. To do so, we use the take() method of batched_dataset() object to store 10% of the data in the test_data variable. \n",
        "4. The remaining data is stored in the train_data object for training using the skip() method.\n",
        "\n",
        "The dataset has been prepared and now we are ready to create our text classification model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WPjb4lrqDL2",
        "colab_type": "text"
      },
      "source": [
        "**Creating the Model**\n",
        "Now we are all set to create our model. \n",
        "1. To do so, we will create a class named TEXT_MODEL that inherits from the tf.keras.Model class. \n",
        "2. Inside the class we will define our model layers.\n",
        "3. Our model will consist of three convolutional neural network layers.\n",
        "4. You can use LSTM layers instead and can also increase or decrease the number of layers.\n",
        "5. I have copied the number and types of layers from SuperDataScience's Google colab notebook and this architecture seems to work quite well for the  dataset as well.\n",
        "\n",
        "Let's now create out model class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW6EVpKDT2xP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TEXT_MODEL(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocabulary_size,\n",
        "                 embedding_dimensions=128,\n",
        "                 cnn_filters=50,\n",
        "                 dnn_units=512,\n",
        "                 model_output_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                 name=\"text_model\"):\n",
        "        super(TEXT_MODEL, self).__init__(name=name)\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocabulary_size,\n",
        "                                          embedding_dimensions)\n",
        "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=2,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=3,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=4,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        if model_output_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,\n",
        "                                           activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=model_output_classes,\n",
        "                                           activation=\"softmax\")\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        l = self.embedding(inputs)\n",
        "        l_1 = self.cnn_layer1(l) \n",
        "        l_1 = self.pool(l_1) \n",
        "        l_2 = self.cnn_layer2(l) \n",
        "        l_2 = self.pool(l_2)\n",
        "        l_3 = self.cnn_layer3(l)\n",
        "        l_3 = self.pool(l_3) \n",
        "        \n",
        "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
        "        concatenated = self.dense_1(concatenated)\n",
        "        concatenated = self.dropout(concatenated, training)\n",
        "        model_output = self.last_dense(concatenated)\n",
        "        \n",
        "        return model_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KDmgoHAqaV4",
        "colab_type": "text"
      },
      "source": [
        "The above script is pretty straightforward. \n",
        "1. In the constructor of the class, we initialze some attributes with default values. \n",
        "2. These values will be replaced later on by the values passed when the object of the TEXT_MODEL class is created.\n",
        "\n",
        "3. Next, three convolutional neural network layers have been initialized with the kernel or filter values of 2, 3, and 4, respectively.\n",
        "\n",
        "4. Again, we can change the filter sizes if we want.\n",
        "\n",
        "5. Next, inside the call() function, global max pooling is applied to the output of each of the convolutional neural network layer.\n",
        "6. Finally, the three convolutional neural network layers are concatenated together and their output is fed to the first densely connected neural network.\n",
        "7. The second densely connected neural network is used to predict the output sentiment since it only contains 2 classes.\n",
        "8. In case you have more classes in the output, you can updated the output_classes variable accordingly.\n",
        "\n",
        "Let's now define the values for the hyper parameters of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHUzPMUGklIo",
        "colab_type": "text"
      },
      "source": [
        "SEQ_LEN is a number of lengths of the sequence after tokenizing. It is set to 128. BERT has worked on at max 512 sequence length.\n",
        "\n",
        "here we use VOCAB_LENGTH as SEQ_LEN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6oyr-TaT5lR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_LENGTH = len(tokenizer.vocab)\n",
        "EMB_DIM = 200\n",
        "CNN_FILTERS = 100\n",
        "DNN_UNITS = 256\n",
        "OUTPUT_CLASSES = 2\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "NB_EPOCHS = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEPWiQWDqxg2",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to create an object of the TEXT_MODEL class and pass the hyper paramters values that we defined in the last step to the constructor of the TEXT_MODEL class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZF_qAuST8eO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
        "                        embedding_dimensions=EMB_DIM,\n",
        "                        cnn_filters=CNN_FILTERS,\n",
        "                        dnn_units=DNN_UNITS,\n",
        "                        model_output_classes=OUTPUT_CLASSES,\n",
        "                        dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH86z1kDT_H2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if OUTPUT_CLASSES == 2:\n",
        "    text_model.compile(loss=\"binary_crossentropy\",\n",
        "                       optimizer=\"adam\",\n",
        "                       metrics=[\"accuracy\"])\n",
        "else:\n",
        "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                       optimizer=\"adam\",\n",
        "                       metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB-FPfBKq8OW",
        "colab_type": "text"
      },
      "source": [
        "Finally to train our model, we can use the fit method of the model class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f76S45jrUB32",
        "colab_type": "code",
        "outputId": "2ce9870d-8ff9-4447-c62f-87d8a6fa5ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "text_model.fit(train_data, epochs=NB_EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "42354/42354 [==============================] - 3706s 87ms/step - loss: 0.4225 - accuracy: 0.8064\n",
            "Epoch 2/3\n",
            "42354/42354 [==============================] - 3751s 89ms/step - loss: 0.3773 - accuracy: 0.8327\n",
            "Epoch 3/3\n",
            "42354/42354 [==============================] - 3819s 90ms/step - loss: 0.3391 - accuracy: 0.8527\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f10d870f160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Maw7bgRmUEn9",
        "colab_type": "code",
        "outputId": "9c2ac431-1cda-47e7-d0ab-a88fe4e4fac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "source": [
        "results = text_model.evaluate(test_data)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-0fc0b8d7099d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1016\u001b[0m                 step_num=step):\n\u001b[1;32m   1017\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  Computed output size would be negative: -2 [input_size: 1, effective_filter_size: 4, stride: 1]\n\t [[node text_model/conv1d_5/conv1d (defined at <ipython-input-37-0faea36fcadf>:45) ]] [Op:__inference_test_function_301990]\n\nFunction call stack:\ntest_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpyydxwyrFOt",
        "colab_type": "text"
      },
      "source": [
        "we can use BERT Tokenizer to create word embeddings that can be used to perform text classification"
      ]
    }
  ]
}